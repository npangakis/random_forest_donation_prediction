---
title: "Who Donates? Using Machine Learning to Predict Federal Donation Behavior"
author:
- Nick Pangakis
output: 
  pdf_document:
    number_sections: yes
    toc: yes
urlcolor: blue
editor_options: 
  chunk_output_type: inline
---
\pagebreak

# Executive Summary {-}

Campaign contributions are an important facet of political behavior that affects who gets elected and what types of policy are implemented. When considered as investments and attempts to influence political outcomes, campaign contributions are a critical component of politics. Moreover, campaign contributions are a multi-billion-dollar political industry. From 2000 to 2020, for example, individual donors living in Pennsylvania contributed more than 23 billion dollars to campaigns for federal office. Non-federal contributions and donations by corporations only increase the significance of money in politics. 

Can we predict who will donate money to a political campaign? While social scientists have invested notable time and resources into understanding how to mobilize voters and how to change political attitudes (e.g., Broockman and Kalla 2016; Gerber and Green 2000), significantly less scholarship has investigated optimal strategies for identifying prospective voters and donors. The primary goal of this project is to use machine learning to predict federal campaign contribution behavior in the United States. First, we implement LASSO logistic regression as a baseline model. Second, we implement random forest. Across both methods, the target variable is a binary indicator for whether an individual donated money to a federal political campaign in 2020. The data for our analysis merge federal campaign contribution data, Pennsylvania voting records, governmental socio-demographic county data, and 2016 U.S. presidential election results.

Our findings indicate that LASSO logistic regression and random forest are very effective at predicting donation behavior. LASSO logistic regression correctly classifies 82.7 percent of test cases and random forest correctly classifies 92.8 percent of test cases. Both of these accuracy scores are notably higher than the 74.1 percent no information rate. Nevertheless, LASSO logistic regression is only able to correctly classify 61.9 percent of positive cases. Remarkably, however, random forest correctly classifies 99.9 percent of positive classes, which indicates that it is the superior model by far. As a result, we conclude that random forest is an extremely effective method for predicting campaign behavior. In our conclusion, we discuss implications of our analysis and next steps for the project.

```{r setup,echo = FALSE, include=FALSE,eval=TRUE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=8, fig.height=4,include=FALSE,eval=FALSE,warning=FALSE, message = FALSE)
options(scipen = 2, digits = 3) # controls base R output
if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(tidyverse, skimr, RCurl, 
               data.table,bit64,stringr,readxl,tidyr,haven,purrr,
               splitstackshape,bestglm, glmnet, leaps, car, pROC,
              randomForest,  rattle, pROC, usmap, xtable, ggcorrplot,
              fastDummies, caret, janitor) 
```

# Data Preparation

This study merges data from four sources:

1. First, we utilize data from the [2021 Pennsylvania voter file](https://www.pavoterservices.pa.gov/pages/purchasepafullvoterexport.aspx), which contains comprehensive administrative data on every individual registered to vote (i.e., over 8 million individuals) in the state of Pennsylvania in 2021. The data includes information on all voters including name, sex, date of birth, date of registration, voter status (i.e., active or inactive), registered political party, residential address, and complete voting history since 2012. We also impute the race of each voter using each voter's name and address.

2. Second, we access all individual-level campaign contributions to federal elections (2000-2020) from individuals living in Pennsylvania. This data comes from the [Federal Election Commission (FEC)](https://www.fec.gov/data/browse-data/?tab=bulk-data), which is available for public download. Founded in 1975, the FEC is a federal organization created for the primary purpose of enforcing campaign finance law. In the name of campaign finance transparency, this federal agency has maintained standardized collections of all individual-level contributions for over twenty years. The raw data includes the name of the contributor, the contributor’s zip code, the contributor’s employer, the contributor’s occupation, the date of the contribution, the donation amount, and which candidate or political organization received the donation. The raw donation data is hundreds of millions of observations for the whole country. For example, from 2017 to 2020 alone, there were 153,592,950 donations made to federal campaigns across the country. 

3. Third, we draw on socio-demographic data from the  [American Community Survey (ACS) 2015-2019 (5-year estimates). ](https://www.census.gov/programs-surveys/acs/data.html). The ACS is an ongoing governmental survey that provides important information on a yearly basis about the United States. The survey results help determine how more than $675 billion in federal and state funds are distributed each year. For this analysis, we downloaded county-level data on total population, median income, percent white, percent Black, percent Hispanic, percent noncitizen, and percent college educated.

5. Finally, we use county-level election results from the 2016 U.S. Presidential Election. The data come from the [MIT Election Lab.](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ)

```{r load in raw data and combine to single dataframe - PA voter file}
########## Voter File Data (PA - 2021)
# This block of code combines each county data file into a single dataframe

### Load in raw voter file data (separate .dtas for each county in PA)
# store .dta file names in a list
file_names <- list.files("data/voter_file_county_PA_raw", pattern="*.dta", full.names=TRUE)
# apply read_dta to each dataframe
ls_df <- lapply(file_names, read_dta)
# cast each variable to character
for (i in 1:length(ls_df)){
  ls_df[[i]] <- lapply(ls_df[[i]],as.character)
}
# bind rows of each separate dataframe
voter_file_PA <- bind_rows(ls_df)
```

```{r data preprocessing - PA voter file data}
# load zip code and county data (for crosswalk to add counties)
zip_to_county_data <-
  read_excel(here("data/ZIP_COUNTY_092021.xlsx")) %>%
  select(1, 2) %>% 
  # limit to unique zip codes (bc several zip codes lie in multiple counties)
  distinct_at(vars(ZIP),.keep_all = T)

# data preprocessing
voter_file_PA_clean <- voter_file_PA %>%
  # Select only columns relevant for analysis
  select(first_name = v4,
         last_name = v3,
         gender = v7,
         dob = v8,
         date_of_registration = v9,
         active = v10,
         last_update = v11,
         political_party = v12,
         address_number = v13,
         address_name = v15,
         city = v18,
         state = v19,
         zip_code = v20,
         last_date_voting = v26,
         lat_final,
         long_final,
         predicted_white = predwhi_final,
         predicted_black = predbla_final,
         predicted_hispanic = predhis_final,
         predicted_asian = predasi_final,
         predicted_other = predoth_final,
         # voting breakdown: AP: at polls, AB: absentee, MB: drop-box
         primary_2012 = p2012,
         general_2012 = g2012,
         primary_2013 = p2013,
         general_2013 = g2013,
         primary_2014 = p2014,
         general_2014 = g2014,
         primary_2015 = p2015,
         general_2015 = g2015,
         primary_2016 = p2016,
         general_2016 = g2016,
         primary_2017 = p2017,
         general_2017 = g2017,
         primary_2018 = p2018,
         general_2018 = g2018,
         primary_2019 = p2019,
         general_2019 = g2019,
         primary_2020 = p2020,
         general_2020 = g2020) %>% 
  # combine address into single column
  unite(address, c("address_number","address_name"), sep =" ")  %>% 
  mutate(# only keep first 3 letters of first name to make joining data easier later
    first_name = str_sub(first_name, 1, 3),
    # Fix formatting of zip_code (Leading '0' dropped when loading)
    zip_code = case_when(nchar(zip_code) == 4 ~ paste0("0", zip_code),TRUE ~ zip_code),
    # fix formatting of zip_code to be first 5 characters
    zip_code = substr(zip_code, 1, 5)) %>% 
  # add column for county fips code
  left_join(zip_to_county_data,by = c("zip_code" = "ZIP"))

```

```{r load in raw data and combine to single dataframe - FEC}
########## FEC Campaign Contribution Data (2000-2020) 
# This block of code combines each year of donation data and combines into a single dataframe

########## Load in raw fec data
# store .dta file names in a list
file_names <- list.files("data/fec_donations_raw", pattern="*.txt", full.names=TRUE)
# apply fread to each dataframe
ls_df_fec <- lapply(file_names, fread)
# filter for just PA
ls_df_fec <- map(ls_df_fec, ~filter(., V10 == "PA"))
# cast each variable to character
for (i in 1:length(ls_df_fec)){
  ls_df_fec[[i]] <- lapply(ls_df_fec[[i]],as.character)
}
# bind rows of each separate dataframe
fec_raw <- bind_rows(ls_df_fec)
# add header data
header <- fread("data/fec_donations_raw/indiv_header_file.csv")
colnames(fec_raw) <- colnames(header)
```

```{r data preprocessing - fec data}
# FEC data preprocessing: step 1
fec_cleaned <- fec_raw %>%
  # Select only columns relevant for analysis
  select(CMTE_ID,NAME,CITY,ZIP_CODE,EMPLOYER,OCCUPATION,TRANSACTION_DT,TRANSACTION_AMT) %>% 
  # data wrangling
  mutate(
      # cast date to character
      TRANSACTION_DT = as.character(TRANSACTION_DT),
      # create year variable
      year = str_sub(TRANSACTION_DT, -4, -1),
      # Fix formatting of zip_code (Leading '0' dropped when loading)
      ZIP_CODE = case_when(nchar(ZIP_CODE) == 4 ~ paste0("0", ZIP_CODE),TRUE ~ ZIP_CODE),
      # fix formatting of zip_code to be first 5 characters
      ZIP_CODE = substr(ZIP_CODE, 1, 5),
      # cast donation amount to int
      TRANSACTION_AMT = as.integer(TRANSACTION_AMT)) %>% 
  # separate first name and last name
  separate(NAME, into = c("last_name","first_name"),sep =",") %>% 
  # only keep first 3 letters of first name (and trim white space)
  # This will make joining data easier later
  mutate(first_name = trimws(str_sub(first_name, 1, 4))) %>% 
  group_by(year, first_name, last_name, ZIP_CODE) %>% 
  # combine multiple donations by single donors
  summarize(# sum donation amount per donor per year
            donation_amount = sum(TRANSACTION_AMT),
            # combine occupations and employers into single observation per donor
            # this is because some donors make multiple donations a year and sometimes will put different occupations/employers
            OCCUPATION = paste(OCCUPATION,collapse = " | "),
            EMPLOYER = paste(EMPLOYER,collapse = " | ")) %>% 
  ungroup() %>% 
  # create variables for donation amount per year
  mutate(donation_amount_2000 = case_when(year == "2000" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2001 = case_when(year == "2001" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2002 = case_when(year == "2002" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2003 = case_when(year == "2003" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2004 = case_when(year == "2004" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2005 = case_when(year == "2005" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2006 = case_when(year == "2006" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2007 = case_when(year == "2007" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2008 = case_when(year == "2008" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2009 = case_when(year == "2009" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2010 = case_when(year == "2010" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2011 = case_when(year == "2011" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2012 = case_when(year == "2012" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2013 = case_when(year == "2013" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2014 = case_when(year == "2014" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2015 = case_when(year == "2015" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2016 = case_when(year == "2016" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2017 = case_when(year == "2017" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2018 = case_when(year == "2018" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2019 = case_when(year == "2019" ~ donation_amount,TRUE ~as.integer(0)),
         donation_amount_2020 = case_when(year == "2020" ~ donation_amount,TRUE ~as.integer(0))) %>% 
  group_by(first_name, last_name, ZIP_CODE) %>% 
  # sum total donation amount per year (so each donor has a single row)
  summarize(donation_amount_2000 = sum(donation_amount_2000),
            donation_amount_2001 = sum(donation_amount_2001),
            donation_amount_2002 = sum(donation_amount_2002),
            donation_amount_2003 = sum(donation_amount_2003),
            donation_amount_2004 = sum(donation_amount_2004),
            donation_amount_2005 = sum(donation_amount_2005),
            donation_amount_2006 = sum(donation_amount_2006),
            donation_amount_2007 = sum(donation_amount_2007),
            donation_amount_2008 = sum(donation_amount_2008),
            donation_amount_2009 = sum(donation_amount_2009),
            donation_amount_2010 = sum(donation_amount_2010),
            donation_amount_2011 = sum(donation_amount_2011),
            donation_amount_2012 = sum(donation_amount_2012),
            donation_amount_2013 = sum(donation_amount_2013),
            donation_amount_2014 = sum(donation_amount_2014),
            donation_amount_2015 = sum(donation_amount_2015),
            donation_amount_2016 = sum(donation_amount_2016),
            donation_amount_2017 = sum(donation_amount_2017),
            donation_amount_2018 = sum(donation_amount_2018),
            donation_amount_2019 = sum(donation_amount_2019),
            donation_amount_2020 = sum(donation_amount_2020),
            OCCUPATION = paste(OCCUPATION,collapse = " | "),
            EMPLOYER = paste(EMPLOYER,collapse = " | ")) %>% 
  ungroup() %>% 
  # add row indicator
  rowid_to_column("ID")

##### Helper Functions
# Helper function for calculating mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Helper function to calculate mode employer and occupation per donor
mode_occupation_employer <- function(data){
  
  # Calculate mode of occupation per donor - so each donor has one occupation/employer
  occupation <- data %>% 
    mutate(OCCUPATION = case_when(OCCUPATION == "" ~ "NONE",TRUE ~ OCCUPATION)) %>% 
    cSplit("OCCUPATION", sep= " | ") %>% 
    pivot_longer(cols = starts_with("OCCUPATION"),names_to = "occupation", values_to = "occupation_name") %>% 
    filter(!is.na(occupation_name)) %>% 
    group_by(ID,first_name,last_name,EMPLOYER) %>% 
    mutate(occupation_name = str_remove(occupation_name,"\\| "),
           occupation_name = str_remove(occupation_name," \\|"),
           occupation_mode = getmode(occupation_name)) %>% 
    distinct(ID,first_name,last_name,EMPLOYER,occupation_mode) %>% 
    ungroup() %>% 
    mutate(occupation_mode = case_when(occupation_mode == "" ~ "NONE",TRUE ~ occupation_mode)) %>%
    select(ID, occupation_mode)
  
  # Calculate mode of employer per donor
  employer <- data %>% 
    mutate(EMPLOYER = case_when(EMPLOYER == "" ~ "NONE",TRUE ~ EMPLOYER)) %>% 
    cSplit("EMPLOYER", sep= " | ") %>% 
    pivot_longer(cols = starts_with("EMPLOYER"),names_to = "employer", values_to = "employer_name") %>% 
    filter(!is.na(employer_name)) %>% 
    group_by(ID,first_name,last_name,OCCUPATION) %>% 
    mutate(employer_name = str_remove(employer_name,"\\| "),
           employer_name = str_remove(employer_name," \\|"),
           employer_mode = getmode(employer_name)) %>% 
    distinct(ID,first_name,last_name,OCCUPATION,employer_mode) %>% 
    ungroup() %>% 
    mutate(employer_mode = case_when(employer_mode == "" ~ "NONE",TRUE ~ employer_mode))  %>% 
    select(ID, employer_mode)
  
  # join together employer and occupation
  employer_occupation <- occupation %>% 
    left_join(employer, by ="ID")
  
  # join employer_occupation to fec
  data_full <- data %>% 
    left_join(employer_occupation, by = "ID") %>% 
    select(-EMPLOYER,-OCCUPATION)
  
  return(data_full)
}

# run function to calculate mode employer and occupation per donor
# need to split data into smaller pieces so R doesnt crash
fec_cleaned$group <- 1:length(fec_cleaned[[1]]) %% 40 + 1
dfList <- split(fec,fec$group)
output <- list()
for (i in 1:length(dfList)){
  output[[i]] <- mode_occupation_employer(dfList[[i]])
}
fec_clean <- bind_rows(output)
```

```{r data preprocessing: ACS data}
########## Socio-demographic Data: American Community Survey (ACS) 2015-2019 (5-year estimates)
# Load in data
ACS2019_raw <- read_csv("data/acs2015_2019_raw.csv")

# data preprocessing
ACS2019_clean <- ACS2019_raw %>% 
  select(state = Geo_STUSAB,
    FIPS = contains("FIPS"),
    total_population = contains("B01003001"), 
         white = contains("B02001002"), 
         african_american = contains("B02001003"), 
         hispanic = contains("B03003003"), 
         noncitizen = contains("B05001006"),
         foreignborn = contains("B05012003"), 
         total_schooling = contains("B15003001"), 
         bach_degree = contains("B15003022"),
         ma_degree = contains("B15003023"), 
         prof_degree = contains("B15003024"), 
         phd_degree = contains("B15003025"), 
         median_income = contains("B19013001") 
         ) %>% 
  mutate(# turn variables into percentages
         white_prop = white1/total_population1,
         aa_prop = african_american1/total_population1,
         hispanic_prop = hispanic1/total_population1,
         noncitizen_prop = noncitizen1/total_population1,
         foreign_prop = foreignborn1/total_population1,
         collegeandabove_prop = (bach_degree1+ma_degree1+prof_degree1+
                                   phd_degree1)/total_schooling1
         ) %>% 
  select(state,FIPS,total_population1,white_prop,aa_prop,hispanic_prop,
         noncitizen_prop,foreign_prop,collegeandabove_prop,median_income1)
```

```{r data loading and preprocessing: 2016 County-Level Presidential Results}
########## 2016 County-Level U.S. Presidential Election Data
# Load in data
election_pres_raw <-
  read_csv("data/countypres_2000_2020_raw.csv")

# data preprocessing
election_pres_clean_2016 <- election_pres_raw %>%
  # limit election data to only 2016
  filter(year == 2016, candidate %in% c("HILLARY CLINTON", "DONALD TRUMP")) %>% 
  group_by(county_fips, candidate, totalvotes) %>%
  summarize(candidate_total = sum(candidatevotes)) %>% 
  mutate(county_fips = as.character(county_fips),
         vote_share = candidate_total/totalvotes,
         candidate = recode(candidate, "HILLARY CLINTON" = "Clinton", 
                            "DONALD TRUMP" = "TRUMP"),
         county_fips = case_when(nchar(county_fips) == 4 ~ paste0("0", county_fips),TRUE ~ county_fips)
  ) %>% 
  filter(candidate == "TRUMP") %>% # for  now, limit to just TRUMP
  ungroup() %>% 
  select(county_fips,totalvotes,trump_voteshare2016 = "vote_share") %>% 
  left_join(zip_to_county_data,by = c("county_fips" = "COUNTY"))
```

```{r merge data}
########## Merge Data
# Join ACS with Presidential data
election_acs_join <-election_pres_clean_2016 %>%
  left_join(ACS2019_clean,by=c("county_fips"="FIPS")) %>% 
  filter(state == "PA")

# Join voter file with donation data
voter_file_fec_join <- voter_file_PA_clean %>% 
  left_join(fec_clean, by = c("zip_code" = "ZIP_CODE","first_name","last_name"))

# join both together
data_full <- voter_file_fec_join %>% 
  left_join(election_acs_join, by= c("zip_code"="ZIP"))
  
```

```{r Final Data Preprocessing 1}
# final data preprocessing steps

# Retrieve current date to calculate distance from dates in voter file (e.g., date of birth)
today <- as.Date(Sys.Date(), format = "%m/%d/%Y")

# Data preprocessing
data_full_clean <- data_full %>%
  mutate(
         # Calculate total donation amount per donor
         donation_total = rowSums(data_full[,41:61]),
         # Calculate total donation amount through 2019 per donor
         donation_through_2019 = rowSums(data_full[,41:60]),
         # calculate donation sums by presidency
         donation_2000_through_2007 = rowSums(data_full_clean[,33:40]),
         donation_2008_through_2011 = rowSums(data_full_clean[,41:44]),
         donation_2012_through_2015 = rowSums(data_full_clean[,45:68]),
         donation_2016_through_2019 = rowSums(data_full_clean[,49:52]),
         # cast certain variables to factor
         gender = as.factor(gender),
         active = as.factor(active),
         political_party = as.factor(political_party),
         city = as.factor(city),
         # reduce the number of categories for some factors
         # randomForest can only handle 53 levels
         city = fct_lump_min(city,20700),
         political_party = fct_lump_min(political_party,40),
         # calculate age of voter in days
         dob = as.Date(dob, format = "%m/%d/%Y"),
         age = as.integer(difftime(today, dob, units = "days")),
         # calculate days since date of voter registration
         date_of_registration = as.Date(date_of_registration, format = "%m/%d/%Y"),
         time_since_registered = as.integer(difftime(today, date_of_registration, units = "days")),
         # calculate days since date of updating registration form
         last_update = as.Date(last_update, format = "%m/%d/%Y"),
         time_since_update = as.integer(difftime(today, last_update, units = "days")),
         # calculate days since date of last time voting
         last_date_voting = as.Date(last_date_voting, format = "%m/%d/%Y"),
         time_since_vote = as.integer(difftime(today, last_date_voting, units = "days")),
         # binary indicator if an individual donated money in 2016
         donation_2016_binary = as.factor(case_when(donation_amount_2016 > 0 ~ as.integer(1),TRUE ~as.integer(0))),
         # binary indicator if an individual donated money in 2020
         donation_2020_binary = as.factor(case_when(donation_amount_2020 > 0 ~ as.integer(1),TRUE ~as.integer(0))),
         # binary indicator if an individual donated money for the first time in 2020
         new_donor = case_when(donation_through_2019 == 0 & donation_amount_2020 > 1 ~ as.integer(1), TRUE ~ as.integer(0)),
         # fill NA values for employer and occupation with "NONE"
         employer_mode = as.character(employer_mode),
         occupation_mode = as.character(occupation_mode),
         employer_mode = case_when(is.na(employer_mode) ~ "NONE", TRUE ~ employer_mode),
         occupation_mode = case_when(is.na(occupation_mode) ~ "NONE", TRUE ~ occupation_mode),
         employer_mode = as.factor(employer_mode),
         occupation_mode = as.factor(occupation_mode),
         # reduce the number of categories for occupation and employer factors
         occupation_mode = fct_lump_min(occupation_mode,220),
         employer_mode = fct_lump_min(employer_mode,103)) %>%
  # drop variables not needed for analysis
  select(-dob,-date_of_registration,-last_update,-address,-state.x,-state.y,-last_date_voting,-ID,-group,-county_fips,-COUNTY)

# cast voting variables to factor
data_full_clean[,15:32] <- lapply(data_full_clean[,15:32],as.factor)

########## drop duplicates
duplicates  <- data_full_clean %>% 
  group_by(first_name,last_name,zip_code) %>% 
  count() %>% 
  filter(n>1)
# drop duplicates
data_full_clean <- anti_join(data_full_clean, duplicates, by=c("first_name","last_name","zip_code"))

########## Deal with NAs
# For yearly donation amount, replace NA's with 0
data_full_clean[, 33:53][is.na(data_full_clean[, 33:53])] <- 0

### for loop to replace NAs 
for (i in 1:ncol(data_full_clean)) {
  if (class(data_full_clean[[i]]) == "numeric") {
    # replace NA with mean
    data_full_clean[[i]] <-
      case_when(is.na(data_full_clean[[i]]) ~
             mean(data_full_clean[[i]], na.rm = TRUE), TRUE ~
             data_full_clean[[i]])
  } else if (class(data_full_clean[[i]]) == "factor"){
    # replace NA with mode
    data_full_clean[[i]] <-
      case_when(is.na(data_full_clean[[i]]) ~
             getmode(data_full_clean[[i]]), TRUE ~
             data_full_clean[[i]])
    
  } else if (class(data_full_clean[[i]]) == "integer"){
    # replace NA with mean
    data_full_clean[[i]] <-
      case_when(is.na(data_full_clean[[i]]) ~
             as.integer(mean(data_full_clean[[i]], na.rm = TRUE)), TRUE ~
             data_full_clean[[i]])
  } 
}
```

```{r Final Data Preprocessing 2}

# minor additional data cleaning
data_full_clean <- data_full_clean %>% 
  mutate(
         # change employer and occupation to not include 2020 data
         occupation_mode = case_when(new_donor == "1" ~ "none_listed",TRUE ~ occupation_mode),
         employer_mode = case_when(new_donor == "1" ~ "none_listed",TRUE ~ employer_mode),
         # clean up some more of the employer and occupation categories
         occupation_mode = case_when(occupation_mode == "|" | occupation_mode == "INFORMATION REQUESTED" | occupation_mode == "REQUESTED" | occupation_mode == "NOT EMPLOYED" | occupation_mode =="INFORMATION REQUESTED PER BEST EFFORTS" ~ "NONE",TRUE ~occupation_mode),
         employer_mode = case_when(employer_mode == "|" | employer_mode == "INFORMATION REQUESTED" | employer_mode == "INFO REQUESTED" | employer_mode == "REQUESTED" | employer_mode == "NOT EMPLOYED" | employer_mode =="INFORMATION REQUESTED PER BEST EFFORTS" | employer_mode =="N/A" ~ "NONE",TRUE ~employer_mode),
         employer_mode = case_when(employer_mode == "SELF" | employer_mode == "SELF EMPLOYED"  ~ "SELF-EMPLOYED",TRUE ~employer_mode),
         occupation_mode = as.factor(occupation_mode),
         employer_mode = as.factor(employer_mode),
         donation_2020_binary = as.factor(donation_2020_binary),
         # relevel factors
         city = fct_lump_min(city,21759),
         political_party = fct_lump_min(political_party,40),
         occupation_mode = fct_lump_min(occupation_mode,349),
         employer_mode = fct_lump_min(employer_mode,128),
         # calculate donation sums by presidency
         donation_2000_through_2007 = rowSums(data_full_clean[,33:40]),
         donation_2008_through_2011 = rowSums(data_full_clean[,41:44]),
         donation_2012_through_2015 = rowSums(data_full_clean[,45:48]),
         donation_2016_through_2019 = rowSums(data_full_clean[,49:52]),
         # change voting record to be binary
         primary_2012 = case_when(primary_2012 == "" ~ "0",TRUE ~ "1"),
         general_2012 = case_when(general_2012 == "" ~ "0", TRUE ~ "1"),
         primary_2013 = case_when(primary_2013 == "" ~ "0",TRUE ~ "1"),
         general_2013 = case_when(general_2013 == "" ~ "0",TRUE ~ "1"),
         primary_2014 = case_when(primary_2014 == "" ~ "0",TRUE ~ "1"),
         general_2014 = case_when(general_2014 == "" ~ "0",TRUE ~ "1"),
         primary_2015 = case_when(primary_2015 == "" ~ "0",TRUE ~ "1"),
         general_2015 = case_when(general_2015 == "" ~ "0",TRUE ~ "1"),
         primary_2016 = case_when(primary_2016 == "" ~ "0",TRUE ~ "1"),
         general_2016 = case_when(general_2016 == "" ~ "0",TRUE ~ "1"),
         primary_2017 = case_when(primary_2017 == "" ~ "0",TRUE ~ "1"),
         general_2017 = case_when(general_2017 == "" ~ "0",TRUE ~ "1"),
         primary_2018 = case_when(primary_2018 == "" ~ "0",TRUE ~ "1"),
         general_2018 = case_when(general_2018 == "" ~ "0",TRUE ~ "1"),
         primary_2019 = case_when(primary_2019 == "" ~ "0",TRUE ~ "1"),
         general_2019 = case_when(general_2019 == "" ~ "0",TRUE ~ "1")) %>% 
  rownames_to_column() %>% 
  select(-first_name,-last_name,-general_2020,-donation_total,-time_since_vote,-primary_2020,-donation_2016_binary,-time_since_update,-donation_amount_2020,-new_donor)

# cast voting to factor
data_full_clean[,14:29] <- lapply(data_full_clean[,14:29],as.factor)

# write file to csv
#write.csv(data_full_clean, "\\data_final.csv", row.names = FALSE)

```

## Data Preprocessing

The full data cleaning procedures and code can be found in the .Rmd file located on GitHub. On GitHub, we include the finalized clean data. Please note that the raw data for this project is not included on GitHub because the raw data is hundreds of millions of observations stored in over 40 separate files. That being said, the data is all publicly available for download in order to replicate this study from raw data. The FEC data can be found [here](https://www.fec.gov/data/browse-data/?tab=bulk-data). The ACS data can be found [here](https://www.census.gov/programs-surveys/acs/data.html). The election results can be found [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ). The 2021 Pennsylvania voter file can be found [here](https://www.pavoterservices.pa.gov/pages/purchasepafullvoterexport.aspx). 

Brief comments on the data cleaning process:

1. The final data is a merge of FEC campaign contribution data, the PA voter file, ACS socio-demographic county data, and 2016 U.S. presidential election results. In $Table 1$ and $Table 2$ in the Appendix, we display each numeric and categorical variable and provide a brief description of each variable.
2. After loading in the raw data for each year of FEC contributions, we group each donation by first name, last name, and zip code and then aggregate the total donation amount per year. After joining the donation data with the PA voter file, the resulting dataframe is a unique row for each individual and their complete donation and voting history.
3. Because the FEC data only includes an individual's first name, last name, and zip code, we cannot be certain that duplicates in the PA voter file are associated with a unique donation history. As a result, we drop all duplicate records from our data. Of the roughly 8 million voter files in the data, around 400,000 (or 4.6 percent) are duplicates. Again, duplicates were defined as multiple records with the same first name and last name living in the same zip code.
4. Any voter file that did not have any donation records were filled with zeros for donation amount over time. To handle NA values for the other numeric or integer variables, we replaced with the mean value for that variable. To handle NA values for categorical variables, we replaced with the mode value for that variable.
5. Random Forest can only handle factors with 53 or fewer levels. So, we reduced the number of categories for our factors to be fewer than 53.

# Exploratory Data Analysis

```{r Data Exploration: Load data, eval = TRUE}
# load pre-cleaned data to enable faster knitting
data_full_clean <- fread("data_final.csv")

# minor data cleaning to fix issues with fread
data_full_clean <- data_full_clean %>% 
  mutate(gender = as.factor(gender),
         active = as.factor(active),
         political_party = as.factor(political_party),
         city = as.factor(city),
         occupation_mode = as.factor(occupation_mode),
         employer_mode = as.factor(employer_mode),
         donation_2020_binary = as.factor(donation_2020_binary))
# cast voting to factor
data_full_clean[,14:29] <- lapply(data_full_clean[,14:29],as.factor)
```

```{r Data Exploration: top observations}
head(data_full_clean)
```

```{r Data Exploration: Target Variable}
print(dim(data_full_clean))
print(table(data_full_clean$donation_2020_binary))
```

There are 8,169,268 unique voters included in the final cleaned data. For each voter, the data includes 70 features, which are described in greater detail in the Appendix. There are 121,851 PA voters who donated to a federal election in 2020 (i.e., our target variable), which is about 1.49 percent of voters.

```{r Data Exploration: Summary Statistics}
# Summary Statistics
skimtable <- skimr :: skim(data_full_clean)
skimtable_factor <- skimtable %>% 
  select(skim_type,skim_variable,factor.top_counts) %>% 
  filter(skim_type == "factor")
skimtable_numeric <- skimtable %>% 
  select(skim_type,skim_variable,numeric.mean,numeric.sd) %>% 
  filter(skim_type == "numeric")
# Numeric summary stats
table1 <- xtable(skimtable_numeric, caption = "Table 1")
# Categorical summary stats
table2 <- xtable(skimtable_factor)

# include in appendix
```

```{r Data Exploration: Data Visualization (data prep), eval=TRUE}
# load zip code and county data (for crosswalk to add counties)
zip_to_county_data <-
  read_excel("ZIP_COUNTY_092021.xlsx") %>%
  select(1, 2) %>% 
  # limit to unique zip codes (bc several zip codes lie in multiple counties)
  distinct_at(vars(ZIP),.keep_all = T)

# join full data with zip code data to add zip code
data_full_clean <- data_full_clean %>% 
  # add column for county fips code
  left_join(zip_to_county_data,by = c("zip_code" = "ZIP")) 
```

```{r Data Exploration: Data Visualization (correlation plot), include=TRUE,eval=TRUE}
# correlation matrix
corr_data <- data_full_clean %>% 
  select(donation_2020_binary,predicted_white,predicted_black,median_income1,collegeandabove_prop,age,donation_2000_through_2007,
         donation_2008_through_2011,donation_2012_through_2015,donation_2016_through_2019) %>% 
  mutate(donation_2020_binary = as.numeric(donation_2020_binary))
### Correlation matrix
corr <- round(cor(corr_data, use = "complete.obs"), 2)
ggcorrplot(corr, type = "lower", lab = TRUE,
           outline.col = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("#E46726", "white", "#6D9EC1"), 
           lab_col = "black", lab_size = 2, 
           tl.cex = 8, tl.col = "black")
```

The above correlation matrix shows that various features are associated with donating in 2020. Past donation behavior, county-level median income, county-level percent college educated, donor's age, and donor's predicted race as white are all positive associated with donating to a federal campaign in 2020. Donor's predicted race as black is negatively associated with donating to a federal campaign in 2020. 

```{r Data Exploration: Data Visualization (donation plot), echo = FALSE, include=TRUE,eval=TRUE}
# Map of donors by county
map <- data_full_clean %>% 
  group_by(COUNTY) %>% 
  count(donation_2020_binary) %>% 
  filter(donation_2020_binary == 1,
         !is.na(COUNTY)) %>% 
  rename(fips = COUNTY)
  
plot_usmap(regions = "counties",labels = F,                
    data = map,include = "PA",
    values = "n", color = "black") + 
    scale_fill_gradient(
      low = "white", high = "red", 
      name = "Number of 2020 Donors", 
      label = scales::comma) + 
    labs(title = "2020 Federal Donation Activity in Pennsylvania") +
    theme(legend.position = "right")
```

The above plot shows the distribution of 2020 donors across PA. As expected, donors are heavily located in urban areas with higher populations like Philadelphia and Pittsburgh.

```{r Data Exploration: Data Visualization (Past Donations), echo = FALSE, include=TRUE,eval=TRUE}

### scatterplot of Past Donations
data_full_clean %>%
  filter(donation_2016_through_2019 > 0) %>% 
  ggplot(aes(x = donation_2016_through_2019, y = donation_2020_binary)) +
  geom_jitter(height = 0.25, aes(color = donation_2020_binary)) +
  labs(y = "2020 Donation Indicator",
       x = "Total Donation Amount in US Dollars (2016-2019)",
       title = "Relationship between Past Donations and 2020 Donations") +
  theme(legend.position = "None")
```

The above plot shows the relationship between an individual's total donation amount between the years 2016 and 2019 and whether an individual made a donation in 2020. The plot demonstrates that individuals who donated more money in the past are more likely to give in 2020.


```{r Data Exploration: Data Visualization (occupation plot), echo = FALSE, include=TRUE,eval=TRUE}
### bar plot of occupation
data_full_clean %>%
  filter(occupation_mode != "none_listed" & occupation_mode != "NONE" & occupation_mode != "Other") %>% 
  group_by(occupation_mode) %>%
  count() %>% 
  filter(n>450) %>% 
  ggplot(aes(x = forcats::fct_reorder(occupation_mode,n), y = n, fill = occupation_mode)) +
  geom_col() +
  labs(y = "Number of Donors",
       x = "Occupation",
       title = "Donors' Occupation Status") +
  theme(legend.position = "None",
        axis.text.x = element_text(
          angle = 90,
          vjust = 0.5,
          hjust = 1)) +
  coord_flip()
```

The above plot shows the listed occupation of donor's included in the data. Other than "none listed," the most popular occupation for donors is retired, which is then followed by attorneys and physicians. 

# Data Analysis

For computational efficiency, we take a weighted random sample of the full dataset. After splitting the data into training and testing sets, the remaining sample for each set is 135,000. For both training and testing data, there are 35,000 instances of 2020 donors and 100,000 instances of non-donors.

```{r Final Data Preparation, eval=TRUE}
set.seed(1)

##### Train-test split
# create functions for Train-test split
test_split <- function(data,n1,n2) {
  # Test split
  test_predictors <-
    # sample n1 samples from the negative class
    data[sample(which(donation_2020_binary == "0"), n1)]
  test_target <-
    # sample n2 samples from the positive class
    data[sample(which(donation_2020_binary == "1"), n2)]
  # combine data
  test_full <- rbind(test_predictors, test_target)
  return(test_full)
}

train_split <- function(data,n1,n2,test_full) {
  # Train split
  # drop all observations that were used in the test set
  training_full <- anti_join(data, test_full, by = "rowname")
  train_predictors <-
    # sample n1 samples from the negative class
    training_full[sample(which(donation_2020_binary == "0"), n1)]
  train_target <-
    # sample n2 samples from the positive class
    training_full[sample(which(donation_2020_binary == "1"), n2)]
  # combine data
  train_sub <- rbind(train_predictors, train_target)
  train_sub <- train_sub %>% 
    select(-rowname)
  return(train_sub)
}

# Set sampling parameters
n1 <- 100000
n2 <- 35000
# # To test if the code works, please run below:
# n1 <- 1000
# n2 <- 350

# Analysis - target variable: binary indicator for 2020 donation behavior
# test split
data_test_analysis_1 <- test_split(data_full_clean,n1,n2)
# train split
data_train_analysis_1 <- train_split(data_full_clean,n1,n2,data_test_analysis_1)
# drop variables not needed for analysis
data_train_analysis_1 <- data_train_analysis_1 %>% 
  select(-COUNTY,-zip_code,-donation_through_2019)
data_test_analysis_1 <- data_test_analysis_1 %>% 
  select(-rowname,-COUNTY,-zip_code,-donation_through_2019)

```

## Logistic Regression Using LASSO

First, we implement logistic regression using LASSO. Logistic regression uses a set of predictors to capture the probability that an event occurs. For this analysis, we have 65 predictors (i.e., features) and the target variable is a binary indicator for whether an individual donated money to a federal campaign in 2020. To avoid overfitting, we implement LASSO (i.e., “Least Absolute Shrinkage and Selection Operator”), which uses k-fold cross-validation and a penalty term for additional features. The goal of LASSO is to return a subset of important variables in order to select a parsimonious model.

```{r Logistic Regression Using LASSO 1, eval=TRUE,include=TRUE}
# create matrix for predictors and target
X <- model.matrix(donation_2020_binary~., data_train_analysis_1)[,-1]
y <- as.matrix(data_train_analysis_1[,"donation_2020_binary"])

# lasso on logistic regression
fit.lasso <- cv.glmnet(X, y, alpha=1, family="binomial", type.measure="deviance")
plot(fit.lasso)
```

The above plot shows the performance of the LASSO logistic regression model as measured by deviance across various values for lambda (i.e., the penalization hyperparameter). The plot shows that adding additional predictive features gradually reduces the deviance. Around log(lambda) = -5.5, however, the model barely reduces its deviance despite adding significantly more variables. As a result, we manually set the lambda term to equal 0.00409 (i.e., exponential(-5.5)), which returns 36 variables.

```{r Logistic Regression Using LASSO 2, eval=TRUE}
# identify features from LASSO
coef <- coef(fit.lasso, s=exp(-5.5))
var_min <- coef@Dimnames[[1]][coef@i + 1][-1]

# create dummy indicators for significant factor levels identified by LASSO
dummy <- function(data){
  # dummy out party 
  dumm_party <- fastDummies::dummy_cols(data$political_party)
  # select only factor levels identified by LASSO
  dumm_party <- dumm_party %>% 
    select(.data_R) %>% 
    rename(party_R = .data_R)
  # dummy out city 
  dumm_city <- fastDummies::dummy_cols(data$city)
  # select only factor levels identified by LASSO
  dumm_city <- dumm_city %>% 
    clean_names() %>% 
    select(data_levittown,data_newtown,data_pittsburgh,data_other,data_warminster,data_wayne,data_state_college) %>% 
    rename(city_LEVITTOWN=data_levittown,city_NEWTOWN=data_newtown,city_PITTSBURGH=data_pittsburgh,city_OTHER=data_other,city_WARMINSTER=data_warminster,city_WAYNE=data_wayne,city_STATE_COLLEGE=data_state_college)
  # dummy out occupation 
  dumm_occupation <- fastDummies::dummy_cols(data$occupation_mode)
  # select only factor levels identified by LASSO
  dumm_occupation <- dumm_occupation %>% 
    select(.data_LABORER,.data_none_listed,.data_Other) %>% 
    rename(occupation_LABORER = .data_LABORER,occupation_nonelisted = .data_none_listed,occupation_other = .data_Other)
  # employer 
  dumm_employer <- fastDummies::dummy_cols(data$employer_mode)
  # select only factor levels identified by LASSO
  dumm_employer <- dumm_employer %>% 
    clean_names() %>% 
    select(data_electricians_local_98,data_highmark_inc) %>% 
    rename(employer_ELECTRICIANS_LOCAL_98 = data_electricians_local_98,
           employer_HIGHMARK_INC = data_highmark_inc)
  
  # combine columns together
  dumm <- cbind(dumm_party,dumm_city,dumm_occupation,dumm_employer)
  return(dumm)
}

# Dummy out training data
dumm_train <- dummy(data_train_analysis_1)
data_train_analysis_1_dummy <- data.frame(cbind(data_train_analysis_1,dumm_train))

# Dummy out testing data
dumm_test <- dummy(data_test_analysis_1)
data_test_analysis_1_dummy <- data.frame(cbind(data_test_analysis_1,dumm_test))

```

```{r Logistic Regression Using LASSO 3, eval=TRUE,include=TRUE}
# fit logistic regression with variables identified from LASSO
fit.lasso.1 <-
  glm(
    donation_2020_binary ~ gender + active + party_R + city_LEVITTOWN + city_NEWTOWN + city_PITTSBURGH +  city_OTHER + city_WARMINSTER + city_WAYNE + city_STATE_COLLEGE + lat_final + predicted_black + predicted_hispanic + predicted_asian + primary_2013 + primary_2015 + primary_2016 + general_2017 + primary_2018 + general_2018 + primary_2019 +  general_2019 + donation_amount_2019 + occupation_LABORER + occupation_nonelisted + occupation_other + employer_ELECTRICIANS_LOCAL_98 + employer_HIGHMARK_INC + foreign_prop + collegeandabove_prop + age + time_since_registered,
    data = data_train_analysis_1_dummy,
    family = binomial
  )

# Create plot for logistic regression results
summary_lasso <- as.data.frame(summary(fit.lasso.1)[["coefficients"]])
summary_lasso <- summary_lasso %>% 
  rownames_to_column("variable") %>% 
  filter(variable != "genderM" & variable != "(Intercept)" & variable != "employer_ELECTRICIANS_LOCAL_98") 
colnames(summary_lasso) <- c("Variable","Estimate","SE","Z","P")
summary_lasso$Estimate <- as.numeric(summary_lasso$Estimate)
summary_lasso$SE <- as.numeric(summary_lasso$SE)

ggplot(summary_lasso, aes(x = forcats::fct_reorder(as.factor(Variable),Estimate), y = Estimate)) +
geom_point(size = 2) +
geom_errorbar(width=.2,size=0.5,aes(ymin = Estimate - (2*SE), ymax = Estimate + (2*SE))) +
geom_hline(yintercept = 0,
           linetype = "longdash",
           color = "red") +
  coord_flip() +
  labs(y = "Estimate",
       x = "Variable",
       title = "Logistic Regression Output Using LASSO")
```

The above plot shows the logistic regression coefficients and their confidence intervals using LASSO. As the plot shows, voters living in counties with higher foreign born populations and higher levels of college education are positively associated with donations in 2020. Voters who are laborers, who voted in 2018 and 2019, and who live in State College, Newtown, Wayne, and Pittsburgh are also more likely to make donations in 2020. Registered Republicans and Asian Americans are all more likely to donate. 

On the other hand, individuals who voted in the 2013 and 2015 primaries, women, Hispanics, African Americans, inactive voters, voters employed by Highmark Inc. and individuals with no occupation listed are less likely to make donations. Voters living in Levittown and Warminster were also less likely to make donations.

```{r Logistic Regression Using LASSO 4, eval=TRUE,include=TRUE}
# create matrix for predictors
X.test <- model.matrix(donation_2020_binary~., data_test_analysis_1_dummy)[,-1]

# Using fit.lasso.1 predict classes of test cases
fit.lasso.pred <- predict(fit.lasso.1, data=X.test, type="response") # estimate prob

# Testing ROC curve
(fit1.roc <- roc(data_test_analysis_1_dummy$donation_2020_binary, fit.lasso.pred, plot=TRUE))

# find optimal threshold
optimal_sensitivity <- max(fit1.roc[["sensitivities"]][(1 - fit1.roc$specificities) < .1])
classifier_threshold <- fit1.roc[["thresholds"]][fit1.roc[["sensitivities"]] == optimal_sensitivity]

```

The above ROC curve informs how well the model is able to distinguish between classes. The curve shows the proportion of classified true positives (y-axis) and false positives (x-axis). The curve is generated by varying the probability threshold for classification. This curve can be used to select an optimal threshold. A classification threshold of 0.36 produces a False Positive rate of less than .1 and a True Positive rate as high as possible. 

```{r, eval=TRUE,include=TRUE,results=TRUE}
# Create confusion matrix
confusionMatrix(data = as.factor(ifelse(fit.lasso.pred>0.36, "1", "0")),
                reference = data_test_analysis_1_dummy$donation_2020_binary,# the confusion table
                positive = levels(data_test_analysis_1_dummy$donation_2020_binary)[2]) 
```

Using held out testing data and a threshold of 0.36, the confusion matrix shown above shows that the LASSO logistic regression model is able to correctly classify 82.7 percent of cases (i.e., 17.3 percent testing error). This is an improvement on the no information rate, which is 0.741. More importantly, the model correctly identifies positive classes (i.e., 2020 donors) at 62 percent. The model correctly identifies negative classes (i.e., non-2020 donors) at 90 percent.

## Random Forest

Second, we implement random forest, which is a machine learning algorithm that combines the logic of bagging and bootstraps. Bagging means combining multiple uncorrelated estimators and a bootstrap involves sampling with replacement from the underlying data to create slightly different samples across estimators. The core idea behind random forest is combining numerous decision trees trained on bootstrap samples for a random subset of features at each split for each tree. The model parameters are tuned using out of bag observations for each tree.

```{r Random Forest: optimize ntree}
# Fit random forest with 500 trees and mtry = p/3
fit.rf <- randomForest(donation_2020_binary~., data_train_analysis_1, mtry=22, ntree=500)    # change ntree
plot(fit.rf, col="red", pch=16, type="p", 
     main="Error Across Trees")
```
![](plot.png)

First, we tune the number of trees to include in the random forest model. The above plot shows out of bag misclassification rate for each class across numerous trees. The results show that the model's error rate stables off around 200 trees. As a result, we use 200 trees for our random forest algorithm.

```{r random forest: optimize mtry}
rf.error.p <- 1:65  # set up a vector of length 65 (i.e., number of features)
for (p in 1:65){ 
  fit.rf2 <- randomForest(donation_2020_binary~., data_train_analysis_1, mtry=p, ntree=200) # 200 because we have already optimized number of trees
  rf.error.p[p] <- fit.rf2$err.rate[200]  # collecting oob mse based on 100 trees
}

plot(1:65, rf.error.p, pch=16,
     main = "Testing errors of mtry with 200 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:65, rf.error.p)
```
![](plot2.png)

Having already optimized the number of trees, we next tune the number of features to randomly sample at each split for each tree. The above plot shows the out of bag misclassification rate across different feature sizes to sample. The results shows that the model's error rate stables off around 20 features. Our final model thus sets mtry to 20 and ntree to 200 for its hyperparameters.

```{r random forest: final model,include=TRUE,results=TRUE,eval=TRUE}
# train final model
fit.rf.final <- randomForest(donation_2020_binary~., data_train_analysis_1, mtry=20, ntree=200) 
```

```{r,include=TRUE,results=TRUE,eval=TRUE}
# prediction using random forest
X.test.rf <- data_test_analysis_1 %>% 
  select(-donation_2020_binary)
predict.rf <- predict(fit.rf.final, newdata=X.test.rf, type="prob")  #probabilities

# Testing ROC curve
(fit.rf.roc <- roc(data_test_analysis_1$donation_2020_binary, predict.rf[,2], plot=TRUE))

# set optimal threshold
optimal_sensitivity <- max(fit.rf.roc[["sensitivities"]][(1 - fit.rf.roc$specificities) < .1])
classifier_threshold <- fit.rf.roc[["thresholds"]][fit.rf.roc[["sensitivities"]] == optimal_sensitivity]

```



```{r,include=TRUE,results=TRUE,eval=TRUE}
# confusion matrix
confusionMatrix(data = as.factor(ifelse(predict.rf[,2]>classifier_threshold, "1", "0")),
                reference = data_test_analysis_1$donation_2020_binary,# the confusion table
                positive = levels(data_test_analysis_1$donation_2020_binary)[2]) 
```

Finally, we assess model performance on held out test data. Looking to the above ROC curve, a classification threshold of 0.032 produces a False Positive rate of less than .1 and a True Positive rate as high as possible. 

Using held out testing data and a threshold of 0.032, the confusion matrix shown above shows that random forest is able to correctly classify 92.9 percent of cases (i.e., 7.1 percent testing error). This is a significant improvement on the no information rate, which is 0.741. Incredibly, the model correctly identifies positive classes (i.e., 2020 donors) at 99.99 percent. Of the 35,000 instances of 2020 donors, random forest correctly classifies 34984 as donors. The model correctly identifies negative classes (i.e., non-2020 donors) at 90.4 percent.

# Conclusion

As a whole, our analyses show that random forest and LASSO logistic regression are very effective at predicting campaign contribution behavior. Comparing logistic regression and random forest, however, reveals that random forest is the superior predictive algorithm. While LASSO logistic regression correctly classifies 82.7 percent of test cases, random forest correctly classifies 92.8 percent of test cases. More importantly, random forest is a remarkable 38 percentage points higher when it comes to positive classification (i.e., 61.9 percent versus 99.9 percent). 

These types of analyses have the power to dramatically reshape the nature of political campaigning. If campaigns target their advertising toward voters who are more likely to donate, then candidates may be more likely to increase their fundraising capabilities. Future iterations of this project will look to expand the data beyond PA and will also aim to make future predictions as test cases, rather than held out data from the same year.

# References

1. Broockman, David, and Joshua Kalla.  2016.  "Durably Reducing Transphobia: A Field Experiment on Door-to-Door Canvassing."  Science (American Association for the Advancement of Science) 352: 220-24.
2. Gerber, Alan S., and Donald P. Green.  2000.  "The Effects of Canvassing, Telephone Calls, and Direct Mail on Voter Turnout: A Field Experiment." The American political science review 94: 653-63.

# Appendix

The below tables list each of the included variables and information on their distribution. For numeric variables, we include a voter's location as latitude and longitude, their predicted race, the amount of money they donated annually from 2000 to 2019, their age, and the time since they initially registered to vote. We also include county-level information on total votes in a county, Trump 2016 vote share, total population, median household income, percent white, percent black, percent Hispanic, percent foreign born, and percent college educated. 

For categorical factors, we include a voter's gender, whether the voter is an active voter or inactive, their registered political party, their home city, their voting history, their occupation, their employer, and a binary indicator for whether they donated in 2020.

\begin{table}[ht]
\centering
\begin{tabular}{rllrr}
  \hline
 & Data Type & Variable & Mean & Standard Deviation \\ 
  \hline
1 & numeric & lat\_final & 40.44 & 0.52 \\ 
  2 & numeric & long\_final & -76.99 & 1.93 \\ 
  3 & numeric & predicted\_white & 0.79 & 0.32 \\ 
  4 & numeric & predicted\_black & 0.10 & 0.24 \\ 
  5 & numeric & predicted\_hispanic & 0.06 & 0.19 \\ 
  6 & numeric & predicted\_asian & 0.03 & 0.13 \\ 
  7 & numeric & predicted\_other & 0.02 & 0.05 \\ 
  8 & numeric & donation\_amount\_2000 & 0.91 & 144.47 \\ 
  9 & numeric & donation\_amount\_2001 & 0.17 & 22.69 \\ 
  10 & numeric & donation\_amount\_2002 & 0.25 & 33.34 \\ 
  11 & numeric & donation\_amount\_2003 & 0.93 & 66.62 \\ 
  12 & numeric & donation\_amount\_2004 & 2.07 & 492.36 \\ 
  13 & numeric & donation\_amount\_2005 & 1.04 & 92.26 \\ 
  14 & numeric & donation\_amount\_2006 & 1.90 & 291.66 \\ 
  15 & numeric & donation\_amount\_2007 & 1.35 & 104.48 \\ 
  16 & numeric & donation\_amount\_2008 & 3.19 & 435.17 \\ 
  17 & numeric & donation\_amount\_2009 & 1.49 & 353.06 \\ 
  18 & numeric & donation\_amount\_2010 & 2.63 & 246.59 \\ 
  19 & numeric & donation\_amount\_2011 & 1.63 & 157.50 \\ 
  20 & numeric & donation\_amount\_2012 & 4.53 & 523.45 \\ 
  21 & numeric & donation\_amount\_2013 & 1.36 & 132.86 \\ 
  22 & numeric & donation\_amount\_2014 & 2.12 & 269.49 \\ 
  23 & numeric & donation\_amount\_2015 & 2.84 & 201.13 \\ 
  24 & numeric & donation\_amount\_2016 & 6.42 & 492.56 \\ 
  25 & numeric & donation\_amount\_2017 & 3.43 & 270.96 \\ 
  26 & numeric & donation\_amount\_2018 & 6.20 & 494.74 \\ 
  27 & numeric & donation\_amount\_2019 & 4.67 & 261.25 \\ 
  28 & numeric & totalvotes & 290694.52 & 239638.46 \\ 
  29 & numeric & trump\_voteshare2016 & 0.49 & 0.17 \\ 
  30 & numeric & total\_population & 593427.88 & 494490.33 \\ 
  31 & numeric & white\_prop & 0.81 & 0.16 \\ 
  32 & numeric & aa\_prop & 0.11 & 0.12 \\ 
  33 & numeric & hispanic\_prop & 0.07 & 0.06 \\ 
  34 & numeric & noncitizen\_prop & 0.03 & 0.02 \\ 
  35 & numeric & foreign\_prop & 0.07 & 0.04 \\ 
  36 & numeric & collegeandabove\_prop & 0.32 & 0.10 \\ 
  37 & numeric & median\_income & 64043.25 & 14885.95 \\ 
  38 & numeric & age (days) & 18893.09 & 6862.34 \\ 
  39 & numeric & time\_since\_registered & 7113.92 & 6159.53 \\ 
  40 & numeric & donation\_2000\_through\_2007 & 8.63 & 830.54 \\ 
  41 & numeric & donation\_2008\_through\_2011 & 8.94 & 826.03 \\ 
  42 & numeric & donation\_2012\_through\_2015 & 10.85 & 867.46 \\ 
  43 & numeric & donation\_2016\_through\_2019 & 20.71 & 1187.79 \\ 
   \hline
\end{tabular}
\caption{Summary Statistics (Numeric Variables)}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rlll}
  \hline
 & Data Type & Variable & Factor Levels \\ 
  \hline
1 & factor & gender & F: 3277955, M: 2785152, U: 1171390, emp: 934771 \\ 
  2 & factor & active & A: 7619828, I: 549439 \\ 
  3 & factor & political\_party & D: 3772313, R: 3199458, NF: 831366, I: 104041 \\ 
  4 & factor & city & Oth: 4642799, PHI: 968617, PIT: 488389, REA: 121211 \\ 
  5 & factor & primary\_2012 & 0: 7021681, 1: 1147587 \\ 
  6 & factor & general\_2012 & 1: 4136508, 0: 4032760 \\ 
  7 & factor & primary\_2013 & 0: 7258968, 1: 910300 \\ 
  8 & factor & general\_2013 & 0: 6798897, 1: 1370371 \\ 
  9 & factor & primary\_2014 & 0: 7129242, 1: 1040026 \\ 
  10 & factor & general\_2014 & 0: 5425306, 1: 2743962 \\ 
  11 & factor & primary\_2015 & 0: 6949836, 1: 1219432 \\ 
  12 & factor & general\_2015 & 0: 6355153, 1: 1814115 \\ 
  13 & factor & primary\_2016 & 0: 5417870, 1: 2751398 \\ 
  14 & factor & general\_2016 & 1: 5039462, 0: 3129806 \\ 
  15 & factor & primary\_2017 & 0: 6952414, 1: 1216854 \\ 
  16 & factor & general\_2017 & 0: 6277553, 1: 1891715 \\ 
  17 & factor & primary\_2018 & 0: 6780993, 1: 1388275 \\ 
  18 & factor & general\_2018 & 1: 4373075, 0: 3796193 \\ 
  19 & factor & primary\_2019 & 0: 6679810, 1: 1489458 \\ 
  20 & factor & general\_2019 & 0: 5694848, 1: 2474420 \\ 
  21 & factor & occupation\_mode & NON: 8011357, Oth: 72392, RET: 36782, ATT: 6803 \\ 
  22 & factor & employer\_mode & NON: 8016837, Oth: 94376, RET: 30859, SEL: 15422 \\ 
  23 & factor & donation\_2020\_binary & 0: 8047417, 1: 121851 \\ 
   \hline
\end{tabular}
\caption{Summary Statistics (Categorical Variables)}
\end{table}




